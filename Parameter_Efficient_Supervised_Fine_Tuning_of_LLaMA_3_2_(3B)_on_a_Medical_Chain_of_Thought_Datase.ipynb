{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c6c52aa75657403486fba0e44eeaac5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d54785532c3d4d4ca7b15dab0216737b"
            ],
            "layout": "IPY_MODEL_ae78ce25db0d4cfa8432cfc520fd2188"
          }
        },
        "9759ad656426460c8772d7dccce57063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81b8021e8d2848d3b832024e0d4b5aff",
            "placeholder": "​",
            "style": "IPY_MODEL_c9605dee0f3a49dda7b36fa0e03dc782",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "9d4874088dbc4f76863d3d7a2e76868e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b358073a4b804912927ce86318fd70d2",
            "placeholder": "​",
            "style": "IPY_MODEL_51780cd8cae44418a986dfbbfd3206a5",
            "value": ""
          }
        },
        "31567f338e4c4b218c4b3bd5ca117324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_cd0303c5d2cf499f879f90ee6051a602",
            "style": "IPY_MODEL_de449717f4d94aa88ad4168b617ec5dd",
            "value": true
          }
        },
        "2d5aaedb394a48999dbde1182fe3e55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2ae5e6dcec374b78be59e8a8d9c6d6c0",
            "style": "IPY_MODEL_bc51f410061b4e198c1c0d9aef21cf76",
            "tooltip": ""
          }
        },
        "13ffa37fdb274638a7b345733da23ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e693aeb40848829d28db5d4ed3c901",
            "placeholder": "​",
            "style": "IPY_MODEL_8d8a265963fb43bab8615f788e6b8524",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ae78ce25db0d4cfa8432cfc520fd2188": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "81b8021e8d2848d3b832024e0d4b5aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9605dee0f3a49dda7b36fa0e03dc782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b358073a4b804912927ce86318fd70d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51780cd8cae44418a986dfbbfd3206a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd0303c5d2cf499f879f90ee6051a602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de449717f4d94aa88ad4168b617ec5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ae5e6dcec374b78be59e8a8d9c6d6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc51f410061b4e198c1c0d9aef21cf76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "95e693aeb40848829d28db5d4ed3c901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d8a265963fb43bab8615f788e6b8524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d856ab1dc93f4f589363c745bf871c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8df6568f0c544cc4a8f71e970891a146",
            "placeholder": "​",
            "style": "IPY_MODEL_f63da3ec337246a7884d8a0c1f257a7e",
            "value": "Connecting..."
          }
        },
        "8df6568f0c544cc4a8f71e970891a146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63da3ec337246a7884d8a0c1f257a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fea2faf47b06433f8dd81f50c1d559fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a318bfa9fda47f9926ae4b6a2c25728",
            "placeholder": "​",
            "style": "IPY_MODEL_e0f3d33d9198443480200ef0bfa35609",
            "value": "Connecting..."
          }
        },
        "1a318bfa9fda47f9926ae4b6a2c25728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f3d33d9198443480200ef0bfa35609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d54785532c3d4d4ca7b15dab0216737b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b511a112b5442849d6ccc67331c23df",
            "placeholder": "​",
            "style": "IPY_MODEL_85d1a780f7934affb5fb479f155e3c2e",
            "value": "Token Project_Internship not found in /root/.cache/huggingface/stored_tokens"
          }
        },
        "0b511a112b5442849d6ccc67331c23df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d1a780f7934affb5fb479f155e3c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0bea6349b04486ab9a21049e21f4145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0060d6e7aa5645ee8e5162c88c7401c8",
              "IPY_MODEL_9cec3f40c4284c52995a1acdb49da29a",
              "IPY_MODEL_b4ba45d5a2e945558b738c6138c966d7"
            ],
            "layout": "IPY_MODEL_7aa104f8b08342449d9ed1991e3568da"
          }
        },
        "0060d6e7aa5645ee8e5162c88c7401c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_873f17c44c1948f99e48f41d374276c5",
            "placeholder": "​",
            "style": "IPY_MODEL_2e8bd7ce88fa4bd08507b3b94d0d950b",
            "value": "Map: 100%"
          }
        },
        "9cec3f40c4284c52995a1acdb49da29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ba9d91124a247129534b1a6358e89d8",
            "max": 19704,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6016af2a16a4a99817199110376d082",
            "value": 19704
          }
        },
        "b4ba45d5a2e945558b738c6138c966d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2299a8f9ea694845aacec2451b9b49ec",
            "placeholder": "​",
            "style": "IPY_MODEL_1a8e053f89c34c19bfd1e36e7b6b4ffa",
            "value": " 19704/19704 [01:42&lt;00:00, 221.18 examples/s]"
          }
        },
        "7aa104f8b08342449d9ed1991e3568da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "873f17c44c1948f99e48f41d374276c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8bd7ce88fa4bd08507b3b94d0d950b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ba9d91124a247129534b1a6358e89d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6016af2a16a4a99817199110376d082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2299a8f9ea694845aacec2451b9b49ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8e053f89c34c19bfd1e36e7b6b4ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 02: Parameter-Efficient Supervised Fine-Tuning of LLaMA 3.2 (3B) on a Medical Chain-of-Thought Datase"
      ],
      "metadata": {
        "id": "zSs_oRjZwvvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview\n",
        "\n",
        "This project focuses on fine-tuning a large language model, specifically Meta’s LLaMA 3 (1B parameter variant), for medical reasoning tasks using Chain-of-Thought (CoT) supervision. The goal is to enhance the model's ability to generate detailed, step-by-step reasoning and accurate answers for complex medical questions.\n",
        "\n",
        "## Task and Objectives:\n",
        "**Task:** Train a causal language model to perform complex medical question-answering by following the chain-of-thought approach.\n",
        "\n",
        "**Objective:** Improve the model’s reasoning and answer quality by explicitly teaching it to generate intermediate reasoning steps (CoT) before producing the final answer.\n",
        "\n",
        "**Dataset:** We use the FreedomIntelligence/medical-o1-reasoning-SFT dataset containing medical questions, CoT reasoning, and answers.\n",
        "\n",
        "## Approach:\n",
        "**Model:** Meta LLaMA 3.2 1B, a powerful open-source foundation model for language tasks.\n",
        "\n",
        "**Fine-tuning Method:** Parameter-efficient fine-tuning using LoRA (Low-Rank Adaptation) to adapt the model with fewer trainable parameters and reduced compute requirements.\n",
        "\n",
        "**Tokenization & Preprocessing:** We prepare the input by combining instruction, chain-of-thought reasoning, and target responses, tokenizing them with padding and truncation to fixed lengths.\n",
        "\n",
        "**Training:** Using the Hugging Face Trainer API, we train the model with mixed precision (fp16) for efficiency, evaluate periodically, and save the best checkpoint.\n",
        "\n",
        "**Evaluation:** Splitting data into training and validation sets for monitoring performance and selecting the best model.\n",
        "\n",
        "**Saving:** Finally, the fine-tuned model and tokenizer are saved for reuse and deployment.\n",
        "\n",
        "\n",
        "## Why This Matters:\n",
        "Medical question-answering benefits greatly from detailed, explainable reasoning.\n",
        "\n",
        "Chain-of-Thought fine-tuning trains the model to think stepwise, improving trustworthiness and answer accuracy.\n",
        "\n",
        "LoRA enables fine-tuning large models even on limited hardware by reducing parameter updates.\n",
        "\n",
        "The project demonstrates how to adapt large language models for specialized, high-stakes domains effectively."
      ],
      "metadata": {
        "id": "z-EmGzpl3QC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Required Packages\n",
        "This cell installs the key Python libraries required for your parameter-efficient fine-tuning of the LLaMA 3.2 model using the Unsloth library and supporting tools.\n",
        "\n",
        "**unsloth:** A high-level library optimized for fast and efficient fine-tuning of LLaMA models (especially 1B–8B scale), with support for Low-Rank Adaptation (LoRA).\n",
        "\n",
        "**bitsandbytes:** Enables 8-bit and 4-bit quantized models to save memory during training—key for fitting large models into consumer GPUs.\n",
        "\n",
        "**accelerate:** From Hugging Face, used to abstract hardware management (CPU, GPU, TPU) and distributed training.\n",
        "\n",
        "**trl:** Transformers Reinforcement Learning library; useful if you want to apply reinforcement learning like PPO for alignment (though not strictly needed for SFT).\n",
        "\n",
        "**peft:** Parameter-Efficient Fine-Tuning by Hugging Face, helps in using methods like LoRA to reduce memory and computational footprint.\n",
        "\n",
        "**datasets:** Simplifies loading and preprocessing datasets from Hugging Face Hub or local files.\n",
        "\n",
        "**wandb:** Weights & Biases, an optional experiment tracking tool.\n",
        "\n",
        "\n",
        "This cell prepares Colab/Notebook environment with all the necessary libraries to:\n",
        "\n",
        "Load and quantize a large LLM (LLaMA 3.2) using unsloth and bitsandbytes.\n",
        "\n",
        "Efficiently fine-tune using LoRA with peft.\n",
        "\n",
        "Work with datasets using datasets.\n",
        "\n",
        "Optionally monitor your experiments using wandb.\n",
        "\n"
      ],
      "metadata": {
        "id": "C0-e4cSPw8y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unsloth\n",
        "!pip install -q bitsandbytes accelerate trl peft datasets wandb\n"
      ],
      "metadata": {
        "id": "YjpcZFQBbcKa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upgrading the Transformers Library\n",
        "\n",
        "I’m upgrading the Transformers library to the latest version.\n",
        "\n",
        "The latest versions of transformers:\n",
        "\n",
        "Include bug fixes, performance improvements, and new model support (like LLaMA 3).\n",
        "\n",
        "Are often required by other libraries like unsloth, which expect newer versions of transformers for compatibility.\n",
        "\n",
        "Support the latest features such as TrainingArguments, Trainer, and advanced tokenization logic.\n",
        "\n",
        "If skipped this step and used an older version, I might run into compatibility errors when initializing models or tokenizers later on."
      ],
      "metadata": {
        "id": "KnumsyrMxiOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzVRQ0dHfyn9",
        "outputId": "f991d81e-955f-4fee-a92c-e5be51709eac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logging Into Hugging Face\n",
        "\n",
        "I’m logging into the Hugging Face Hub from this notebook using my personal Hugging Face account.\n",
        "\n",
        "Logging in gives me authenticated access to:\n",
        "\n",
        "Download private or gated models and datasets from Hugging Face (like meta-llama models).\n",
        "\n",
        "Upload my fine-tuned models back to the Hugging Face Hub if I want to share or deploy them.\n",
        "\n",
        "Avoid 403 Forbidden or rate-limit issues when accessing large files hosted on Hugging Face.\n",
        "\n",
        "**What Happens During This Step:**\n",
        "\n",
        "I’ll be prompted to paste my Hugging Face access token\n",
        "\n",
        "Once authenticated, the notebook will store a secure session token to use Hugging Face APIs behind the scenes.\n"
      ],
      "metadata": {
        "id": "axF4rOsgxzd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c6c52aa75657403486fba0e44eeaac5d",
            "9759ad656426460c8772d7dccce57063",
            "9d4874088dbc4f76863d3d7a2e76868e",
            "31567f338e4c4b218c4b3bd5ca117324",
            "2d5aaedb394a48999dbde1182fe3e55e",
            "13ffa37fdb274638a7b345733da23ac9",
            "ae78ce25db0d4cfa8432cfc520fd2188",
            "81b8021e8d2848d3b832024e0d4b5aff",
            "c9605dee0f3a49dda7b36fa0e03dc782",
            "b358073a4b804912927ce86318fd70d2",
            "51780cd8cae44418a986dfbbfd3206a5",
            "cd0303c5d2cf499f879f90ee6051a602",
            "de449717f4d94aa88ad4168b617ec5dd",
            "2ae5e6dcec374b78be59e8a8d9c6d6c0",
            "bc51f410061b4e198c1c0d9aef21cf76",
            "95e693aeb40848829d28db5d4ed3c901",
            "8d8a265963fb43bab8615f788e6b8524",
            "d856ab1dc93f4f589363c745bf871c93",
            "8df6568f0c544cc4a8f71e970891a146",
            "f63da3ec337246a7884d8a0c1f257a7e",
            "fea2faf47b06433f8dd81f50c1d559fc",
            "1a318bfa9fda47f9926ae4b6a2c25728",
            "e0f3d33d9198443480200ef0bfa35609",
            "d54785532c3d4d4ca7b15dab0216737b",
            "0b511a112b5442849d6ccc67331c23df",
            "85d1a780f7934affb5fb479f155e3c2e"
          ]
        },
        "id": "RAdE2YwnbcOB",
        "outputId": "8ffe4d96-e87d-4348-89f9-f35a571e62ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6c52aa75657403486fba0e44eeaac5d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Essential Imports\n",
        "\n",
        "In this step, I’m importing all the core libraries required for model loading, training, fine-tuning, evaluation, and performance boosts.\n",
        "\n",
        "**Line-by-Line Explanation:**\n",
        "**import unsloth**\n",
        "This is crucial to import first, as Unsloth applies performance optimizations early on to speed up and memory-optimize Hugging Face models (especially for LLaMA 3).\n",
        "\n",
        "**from transformers import ...**\n",
        "I'm importing the Hugging Face Transformers components:\n",
        "\n",
        "**AutoTokenizer & AutoModelForCausalLM:** for tokenizing inputs and loading the base LLM.\n",
        "\n",
        "**Trainer, TrainingArguments:** to handle model training and evaluation with built-in functionality.\n",
        "\n",
        "**from datasets import load_dataset**\n",
        "This lets me load Hugging Face datasets easily — in my case, the Medical Chain-of-Thought dataset.\n",
        "\n",
        "**from peft import ...**\n",
        "From the Parameter-Efficient Fine-Tuning (PEFT) library, I import:\n",
        "\n",
        "LoraConfig, get_peft_model, TaskType to apply LoRA (Low-Rank Adaptation) and reduce memory usage and training time.\n",
        "\n",
        "**import evaluate**\n",
        "This allows me to use standard evaluation metrics like accuracy, BLEU, F1, etc.\n",
        "\n",
        "**import torch**\n",
        "The core PyTorch library — required since all Hugging Face models are built on PyTorch.\n",
        "\n",
        "This cell sets up the environment with all the necessary libraries and tools to:\n",
        "\n",
        "Load the LLM model\n",
        "\n",
        "Fine-tune it using LoRA\n",
        "\n",
        "Evaluate it efficiently\n",
        "\n",
        "Optimize performance with unsloth"
      ],
      "metadata": {
        "id": "QFOMfaTJyOiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports (import unsloth BEFORE transformers!)\n",
        "\n",
        "import unsloth  # must be first for speed optimizations\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import evaluate\n",
        "import torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_32vcSg2eXre",
        "outputId": "54310a92-1046-4119-a7fc-3587cd9d93aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "In this step, I’m loading the dataset that I want to fine-tune my model on. Specifically, I’m using the English configuration of a Chain-of-Thought (CoT) medical reasoning dataset.\n",
        "\n",
        "\n",
        "**Line-by-Line Explanation:*8\n",
        "**load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train\")**\n",
        "This downloads and loads the training split of the Hugging Face dataset:\n",
        "\n",
        "**Dataset: FreedomIntelligence/medical-o1-reasoning-SFT**\n",
        "\n",
        "Configuration: \"en\" (English)\n",
        "\n",
        "Split: \"train\" only (for now)\n",
        "\n",
        "**print(dataset.column_names)**\n",
        "This shows me all the column names (fields) in the dataset so I know what data is available — such as \"Question\", \"Complex_CoT\", and \"Response\".\n",
        "\n",
        "**print(dataset[0])**\n",
        "I print the first example in the dataset to visually inspect what a full sample looks like. This helps confirm the format and content, which is critical before preprocessing.\n",
        "\n",
        "This dataset contains structured medical questions, detailed reasoning steps (Chain of Thought), and final answers. It’s ideal for training LLMs on step-by-step medical reasoning, which is valuable for many real-world healthcare AI applications.\n",
        "\n",
        "I’ve successfully loaded and inspected the Medical CoT dataset to understand its structure and contents. This dataset will be used for training the LLaMA 3 model with Chain-of-Thought supervision."
      ],
      "metadata": {
        "id": "sObHMN7tyyO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train\")\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4zYMjvVklnR",
        "outputId": "0c1c2ec8-99dd-4801-d0f9-a2a7d640b356"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Question', 'Complex_CoT', 'Response']\n",
            "{'Question': 'Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?', 'Complex_CoT': \"Okay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\", 'Response': 'The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Loading the LLaMA 3.2 Model & Setting the Tokenizer\n",
        "\n",
        "I'm loading the LLaMA 3.2 1B model tokenizer and making sure it has a padding token. Some Hugging Face tokenizers (especially for causal language models like LLaMA) don’t come with a pad_token by default, so I manually set it to the eos_token.\n",
        "\n",
        "**Line-by-Line Breakdown:**\n",
        "**model_name = \"meta-llama/Llama-3.2-1B\"**\n",
        "I define the model name — this points to the official Hugging Face ID of Meta’s LLaMA 3.2B model (smallest LLaMA 3 variant, good for fast experiments).\n",
        "\n",
        "**AutoTokenizer.from_pretrained(model_name, use_fast=False)**\n",
        "Loads the tokenizer for LLaMA 3.\n",
        "\n",
        "I set use_fast=False because Unsloth sometimes prefers the slow tokenizer for compatibility reasons.\n",
        "\n",
        "**if tokenizer.pad_token is None:**\n",
        "Checks whether a pad_token is defined (most LLaMA tokenizers don't have one by default).\n",
        "\n",
        "**tokenizer.pad_token = tokenizer.eos_token**\n",
        "If it’s missing, I simply set the pad token to the same as the end-of-sequence token (eos_token).\n",
        "This is common practice for causal LMs and makes sure padding works during training.\n",
        "\n",
        "Without a pad_token, training with batches of unequal lengths would fail or throw an error. We need to pad all sequences in a batch to the same length — hence this fix is essential for stable training.\n",
        "\n",
        "I’ve loaded the tokenizer for LLaMA 3.2-1B and fixed a critical issue by assigning a padding token, ensuring compatibility with batching and training.\n"
      ],
      "metadata": {
        "id": "fk1AMWQzzNvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "\n",
        "# Fix if tokenizer has no pad token:\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "9U1K5aVNkmFB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Function for Supervised Fine-Tuning (SFT)\n",
        "\n",
        "In this step, I’m defining the preprocessing logic that will convert raw question–CoT–answer examples into tokenized inputs and labels that the model can learn from.\n",
        "\n",
        "**Step-by-Step Explanation:**\n",
        "**Input Formatting:**\n",
        "Combines each question (\"Question\") with its corresponding chain-of-thought (\"Complex_CoT\") into a structured input string.\n",
        "\n",
        "The format helps the model understand the reasoning path leading to the answer.\n",
        "\n",
        "**Tokenize the Input:**\n",
        "Tokenizes the formatted prompt.\n",
        "\n",
        "Ensures uniform length by padding/truncating to 512 tokens — essential for batching.\n",
        "\n",
        "**Tokenize the Output Labels (Target Text):**\n",
        "Tokenizes the \"Response\" field — this is what the model is supposed to generate.\n",
        "\n",
        "Uses as_target_tokenizer() to distinguish output tokens from input tokens (relevant for Seq2Seq setups).\n",
        "\n",
        "** Apply Padding Mask to Labels:**\n",
        "Sets all padding tokens in the label to -100, which tells PyTorch to ignore these during loss calculation.\n",
        "\n",
        "This is important to avoid penalizing the model for predicting <pad> tokens.\n",
        "\n",
        "**Attach Labels to Input Dict:**\n",
        "Adds the masked label tokens to the model_inputs dictionary.\n",
        "\n",
        "Now, each data point includes both input_ids and labels, ready for training.\n",
        "\n",
        "Training a model with raw text isn't enough — we need to structure and tokenize it in a way that matches how the model processes input and evaluates performance. This function ensures:\n",
        "\n",
        "The model understands the format of reasoning (CoT),\n",
        "\n",
        "The loss is calculated only on relevant parts of the sequence,\n",
        "\n",
        "We have consistent batch sizes (via fixed-length padding).\n",
        "\n",
        "I’ve created a preprocessing function that prepares structured examples for training by tokenizing prompts and responses, applying padding, and masking loss on padded tokens."
      ],
      "metadata": {
        "id": "TkF6F_1_znij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [\n",
        "        f\"Instruction: {q}\\nCoT: {cot}\\nAnswer:\"\n",
        "        for q, cot in zip(examples[\"Question\"], examples[\"Complex_CoT\"])\n",
        "    ]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"Response\"],\n",
        "            max_length=512,           # match input max_length here\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "    # Mask padding tokens with -100 for loss ignoring\n",
        "    labels_with_mask = []\n",
        "    for label_seq in labels[\"input_ids\"]:\n",
        "        labels_with_mask.append([token if token != tokenizer.pad_token_id else -100 for token in label_seq])\n",
        "\n",
        "    model_inputs[\"labels\"] = labels_with_mask\n",
        "    return model_inputs\n"
      ],
      "metadata": {
        "id": "Nbeom3NukoI3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the Dataset Using the Preprocessing Function\n",
        "\n",
        "In this step, I’m applying the preprocess_function (from the previous cell) to every example in the dataset to produce tokenized and model-ready data.\n",
        "\n",
        "**Step-by-Step Explanation:**\n",
        "**dataset.map(...)**\n",
        "This is a Hugging Face datasets function that applies a transformation (in our case, the preprocess_function) to every item in the dataset.\n",
        "\n",
        "It’s efficient and optimized for large datasets.\n",
        "\n",
        "**batched=True**\n",
        "This tells the map function to send a batch of examples at a time into the function, instead of one-by-one.\n",
        "\n",
        "This is faster and compatible with our preprocess_function, which is designed to process lists of examples.\n",
        "\n",
        "**remove_columns=dataset.column_names**\n",
        "After tokenization, the original columns like \"Question\", \"Complex_CoT\", and \"Response\" are no longer needed.\n",
        "\n",
        "This cleans up the dataset so that only tokenized inputs and labels remain — exactly what the model expects.\n",
        "\n",
        "At this point, we’re turning our raw medical CoT dataset into something the LLaMA model can train on. Without this step:\n",
        "\n",
        "The model wouldn't understand the structure or tokenized format.\n",
        "\n",
        "We wouldn’t be able to pass this data into Trainer.\n",
        "\n",
        "I’m now transforming my dataset into tokenized format by mapping the preprocess_function over it. This gives me inputs (input_ids, attention_mask) and corresponding target labels, all ready for training.\n",
        "\n"
      ],
      "metadata": {
        "id": "H6prSG500dDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your original dataset is loaded as `dataset`\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a0bea6349b04486ab9a21049e21f4145",
            "0060d6e7aa5645ee8e5162c88c7401c8",
            "9cec3f40c4284c52995a1acdb49da29a",
            "b4ba45d5a2e945558b738c6138c966d7",
            "7aa104f8b08342449d9ed1991e3568da",
            "873f17c44c1948f99e48f41d374276c5",
            "2e8bd7ce88fa4bd08507b3b94d0d950b",
            "0ba9d91124a247129534b1a6358e89d8",
            "e6016af2a16a4a99817199110376d082",
            "2299a8f9ea694845aacec2451b9b49ec",
            "1a8e053f89c34c19bfd1e36e7b6b4ffa"
          ]
        },
        "id": "VrE1qdymld0j",
        "outputId": "cf439ccf-1ed4-45a1-aa32-e47c4cd597b8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/19704 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0bea6349b04486ab9a21049e21f4145"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Tokenized Dataset into Train and Validation Sets\n",
        "\n",
        "I'm dividing the tokenized dataset into two parts:\n",
        "\n",
        "Training set (train_data): used by the model to learn.\n",
        "\n",
        "Validation set (val_data): used to evaluate how well the model is performing during training (without touching test data).\n",
        "\n",
        " **Explanation of Each Line:**\n",
        "**tokenized_dataset.train_test_split(test_size=0.05, seed=42)**\n",
        "This splits the dataset into:\n",
        "\n",
        "95% for training\n",
        "\n",
        "5% for validation\n",
        "\n",
        "The seed=42 ensures reproducibility — the same split every time you run the code.\n",
        "\n",
        "**train_data and val_data**\n",
        "I’m assigning the resulting splits to two separate variables.\n",
        "\n",
        "These will be passed later to the Trainer for training and evaluation.\n",
        "\n",
        "We need separate training and validation sets to detect overfitting and monitor generalization.\n",
        "\n",
        "If we train and evaluate on the same data, we won’t know how well the model performs on unseen examples.\n",
        "\n",
        "I’m now splitting the tokenized data into training and validation sets so the model can learn and be evaluated properly during training."
      ],
      "metadata": {
        "id": "szGPMea-0y4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
        "train_data = split_dataset[\"train\"]\n",
        "val_data = split_dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "bX0PA8GRlEKn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixing the Tokenizer Padding Token\n",
        "\n",
        "I’m checking if the tokenizer has a padding token defined. If it doesn’t, I assign its end-of-sequence (eos) token as the padding token.\n",
        "\n",
        "Many transformer models require a padding token to pad sequences to the same length during batching.\n",
        "\n",
        "Some tokenizers (especially from newer or less common models) do not have a pad token by default.\n",
        "\n",
        "Without a padding token:\n",
        "\n",
        "You might get errors during training or tokenization.\n",
        "\n",
        "Padding behavior will be undefined.\n",
        "\n",
        "\n",
        "**Why choose eos_token as padding?**\n",
        "The eos token typically signals the end of a sequence.\n",
        "\n",
        "It's a safe fallback since it already exists in the tokenizer’s vocabulary.\n",
        "\n",
        "This avoids adding new tokens and changing tokenizer/model embeddings"
      ],
      "metadata": {
        "id": "A50maVjH1Lqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "OO6B-fIpl4oS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Data Collator for Seq2Seq Training\n",
        "\n",
        "I’m creating a data collator that dynamically batches and pads inputs during training and evaluation.\n",
        "\n",
        "During training, input sequences usually have different lengths.\n",
        "\n",
        "To feed them in batches, sequences need to be padded to the same length.\n",
        "\n",
        "This data collator:\n",
        "\n",
        "Handles dynamic padding (pads only as much as needed per batch).\n",
        "\n",
        "Prepares batches with correct padding tokens and labels.\n",
        "\n",
        "Supports seq2seq tasks, so it properly manages both inputs and target sequences (labels).\n",
        "\n",
        "\n",
        "\n",
        "**Why use DataCollatorForSeq2Seq instead of a generic collator?**\n",
        "This collator is specifically designed for sequence-to-sequence models (e.g., translation, text generation with target outputs).\n",
        "\n",
        "It makes sure that:\n",
        "\n",
        "Padding tokens in labels are masked out properly during loss calculation (usually with -100).\n",
        "\n",
        "Padding is handled correctly for both inputs and labels."
      ],
      "metadata": {
        "id": "BdhMxbDJ1YJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "fg379rD5l6eB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Training Arguments\n",
        "\n",
        "I’m configuring the training setup for the model by specifying all the important hyperparameters and behavior for training.\n",
        "\n",
        "**Why are these parameters important?**\n",
        "**output_dir:** Directory to save model checkpoints and logs.\n",
        "\n",
        "**per_device_train_batch_size & per_device_eval_batch_size:** Batch sizes per GPU/CPU for train and eval.\n",
        "\n",
        "**eval_strategy=\"steps\":** Run evaluation every eval_steps during training.\n",
        "\n",
        "**eval_steps=200:** Frequency of evaluation steps.\n",
        "\n",
        "**logging_steps=100:** Log training stats every 100 steps.\n",
        "\n",
        "**save_steps=400:** Save checkpoint every 400 steps (must be multiple of eval_steps when load_best_model_at_end=True).\n",
        "\n",
        "**save_total_limit=2:** Keep only the last 2 checkpoints to save disk space.\n",
        "\n",
        "**num_train_epochs=3:** Number of times to go through the entire training dataset.\n",
        "\n",
        "**learning_rate=3e-4:** Controls step size in gradient descent; key for training speed and quality.\n",
        "\n",
        "**warmup_steps=100:** Gradually increase learning rate over these initial steps for stability.\n",
        "\n",
        "**logging_dir:** Directory to save tensorboard logs.\n",
        "\n",
        "**report_to=\"none\":** Disable reporting to any external tool like WandB.\n",
        "\n",
        "**load_best_model_at_end=True:** Automatically load the checkpoint with the best evaluation metric at the end of training.\n",
        "\n",
        "**bf16=False:** Disabled because bf16 requires specific hardware (Ampere+ GPUs).\n",
        "\n",
        "**fp16=True:** Enables mixed precision training (faster and less memory usage if hardware supports it).\n",
        "\n",
        "**Why set save_steps as multiple of eval_steps?**\n",
        "Because load_best_model_at_end=True requires checkpoints to be saved at evaluation points so it can correctly pick the best checkpoint.\n"
      ],
      "metadata": {
        "id": "IsY1QRUp1qUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama3_medical_cot_lora\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    logging_steps=100,\n",
        "    save_steps=400,               # Make this multiple of eval_steps (200)\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=3e-4,\n",
        "    warmup_steps=100,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    bf16=False,                  # Set to False unless you have Ampere+ GPU and CUDA 11+\n",
        "    fp16=True,                   # Use fp16 if supported\n",
        ")\n"
      ],
      "metadata": {
        "id": "YXn57cjLl8J8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Trainer\n",
        "\n",
        "I’m creating a Trainer object, which is the core component from Hugging Face Transformers that handles the training loop, evaluation, checkpoint saving, and more — all configured with the parameters and datasets I prepared earlier.\n",
        "\n",
        "**Why is this important?**\n",
        "**model=model:** The model to train (in our case, LLaMA 3 with LoRA).\n",
        "\n",
        "**args=training_args:** The training settings we just defined.\n",
        "\n",
        "**train_dataset=train_data:** The dataset for training.\n",
        "\n",
        "**eval_dataset=val_data:** The dataset for validation during training.\n",
        "\n",
        "**tokenizer=tokenizer:** Tokenizer for converting text to tokens, used for decoding and encoding during training and evaluation.\n",
        "\n",
        "**data_collator=data_collator:** Handles batching and padding of samples dynamically during training (important since sequences may vary in length).\n",
        "\n",
        "**Why use Trainer?**\n",
        "Trainer abstracts away all the complex steps like:\n",
        "\n",
        "Batching & padding sequences,\n",
        "\n",
        "Computing loss,\n",
        "\n",
        "Running backpropagation,\n",
        "\n",
        "Saving checkpoints,\n",
        "\n",
        "Running periodic evaluations, and\n",
        "\n",
        "Logging progress.\n",
        "\n",
        "It lets me focus on what I want to train rather than how to train it.\n"
      ],
      "metadata": {
        "id": "_0y1hS0n2Rb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XodRfeqol-CJ",
        "outputId": "4500cdc1-80bb-4136-c741-28f91f0ff15f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-674c4bda8123>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training\n",
        "\n",
        "This command kicks off the entire training process with all the configurations, data, and model setup we did earlier.\n",
        "\n",
        "**Why is this important?**\n",
        "It runs the forward and backward passes on batches of data.\n",
        "\n",
        "Optimizes the model weights according to the loss function.\n",
        "\n",
        "Performs evaluation at intervals to monitor progress.\n",
        "\n",
        "Saves checkpoints as configured.\n",
        "\n",
        "Applies all training settings like learning rate, batch size, mixed precision (fp16), etc.\n",
        "\n",
        "**What happens during training?**\n",
        "The model processes input sequences, predicts outputs.\n",
        "\n",
        "The loss between predictions and true labels is computed.\n",
        "\n",
        "Gradients are backpropagated to update model weights.\n",
        "\n",
        "This repeats for the specified number of epochs or steps.\n",
        "\n",
        "Validation runs every few steps to check how well the model generalizes.\n",
        "\n"
      ],
      "metadata": {
        "id": "m135Hr6d2rYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "8y2rBE0dmAgf",
        "outputId": "876b245b-f8f9-4f37-eba1-bbd8a3db69fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1805' max='14040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1805/14040 32:38 < 3:41:33, 0.92 it/s, Epoch 0.39/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>6.795900</td>\n",
              "      <td>6.827287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>6.730200</td>\n",
              "      <td>6.734843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>6.647600</td>\n",
              "      <td>6.698766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>6.610800</td>\n",
              "      <td>6.672456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.548300</td>\n",
              "      <td>6.663226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>6.582200</td>\n",
              "      <td>6.640367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>6.576200</td>\n",
              "      <td>6.624905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>6.577300</td>\n",
              "      <td>6.612070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>6.579000</td>\n",
              "      <td>6.601445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Save Fine-Tuned Model and Tokenizer\n",
        "\n",
        "Saving the trained model weights and tokenizer configuration to a local folder after training finishes.\n",
        "\n",
        "**Why is this important?**\n",
        "Preserves the fine-tuned model so you can reuse it later without retraining.\n",
        "\n",
        "Saves tokenizer vocab and special tokens to ensure consistency during inference.\n",
        "\n",
        "Makes it easy to share or deploy your model elsewhere.\n",
        "\n",
        "The saved folder contains all necessary files to reload the model and tokenizer with from_pretrained().\n",
        "\n",
        "**What happens when I reload?**\n",
        "The model weights and architecture load exactly as saved.\n",
        "\n",
        "The tokenizer loads its vocabulary and special tokens.\n",
        "\n",
        "You can immediately run inference or continue training."
      ],
      "metadata": {
        "id": "9SEgLpEF28tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./llama3_medical_cot_lora\")\n",
        "tokenizer.save_pretrained(\"./llama3_medical_cot_lora\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "gvU7QhMrmClR",
        "outputId": "ecca383c-99de-4a7c-aea3-c6334df54554"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6257f4b7fe52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./llama3_medical_cot_lora\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./llama3_medical_cot_lora\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nF07MGvuguV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}